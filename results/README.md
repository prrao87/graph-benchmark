# Graph benchmark results

This directory contains the raw benchmark outputs (per system) and a small script to render them into a comparable table + plot.

## Plot

![](./benchmark_plot.png)

The plot compares the mean latency (lower is better) for queries `q1`–`q9` across
systems. The y-axis is log-scaled to make it easier to see differences across both
fast (single-digit ms) and slow (seconds) queries.

## Regenerate

The chart was generated by copying the table of results from each `pytest-benchmark` run for a given system, and saving it in the `*.txt` files in this directory. The filename indicates the system that the results are for.

Regenerate the plot for the latest results as follows.

```sh
uv run compare.py
```

## Explanation of results

These results reflect several layers of system behavior, not just “the query plan.”

- Most queries have an intuitive logical shape that looks relational: scans over edge tables, joins along the path pattern, optional filters, grouped aggregates, and top-k selection.
- The query planner and optimizer influence performance through join ordering, predicate pushdown, projection pushdown, and limit/top-k optimizations.
- The execution model matters during runtime. Neo4j commonly uses a row-at-a-time iterator approach (often described as a Volcano-style model), while Kuzu/Ladybug and `lance-graph` execute over columnar data structures, which enables vectorized processing, better cache locality, and efficient partial aggregation.
- The storage and access paths also matter. Neo4j’s property graph storage favors pointer-based traversals, whereas Kuzu/Ladybug store properties in columnar form and execute traversal patterns using join-style operators, and `lance-graph` operates on Arrow/Lance columnar datasets.
- Deployment overhead can be visible on shorter queries. Neo4j runs as a server accessed over Bolt, while Kuzu/Ladybug and `lance-graph` are embedded/in-process for this benchmark, which reduces per-query overhead.

## Query-by-query intuition

- `q1` (top 3 most-followed people) is an edge scan over `FOLLOWS` followed by `GROUP BY person` with `COUNT(*)`, and then a top-k selection via `ORDER BY ... LIMIT 3`. A good plan uses partial aggregation, avoids a full sort when possible, and delays fetching wide properties (such as names) until the top-k person IDs are known.
- `q2` (city of the most-followed person) is the same top-k idea with `LIMIT 1`, followed by a single-hop lookup/expand to `LIVES_IN`. The best plans compute the winner first and only then perform the location join, so the expansion stays constant-sized.
- `q3` (five cities with the lowest average age in a given country) has a “filter → join chain → aggregate → top-k” shape: filter on `Country`, join through `State` and `City` to the `LIVES_IN` edges and `Person`, compute `AVG(age)` per city, and select the smallest five. Columnar, vectorized engines tend to do well here because the plan is dominated by joins and grouped aggregation rather than graph-style exploration, which helps explain why `lance-graph` + DataFusion can be fastest on this query.
- `q4` (people aged 30–40 per country, top 3) similarly benefits from applying the `Person.age` filter early, joining through the location chain, and using efficient grouped counting plus a top-k selection. The main sensitivities are filter selectivity and join ordering.
- `q5` (count of male people in London with a given interest) is a selective count with multiple filters across `Interest`, `Person`, and `City`. Strong plans start from whichever predicate is most selective (often the specific city or interest), push filters down, and avoid unnecessary intermediate materialization. In Cypher-like languages, `WITH` clauses can act as planning barriers that limit join reordering, which can make this query more sensitive to planner differences.
- `q6` (top cities by count of female tennis fans) is a filter on `Interest` and `gender` followed by joins to location, grouped counting by city, and top-k. This is another query that maps cleanly to a columnar “scan/join/aggregate/top-k” pipeline.
- `q7` (top U.S. state by count of people aged 23–30 who like photography) combines filters on age, country, and interest with a join chain, grouped counting by state, and top-1 selection. Join ordering and early filter application strongly influence how much intermediate work is done.
- `q8` (count of length-2 `FOLLOWS` paths) and `q9` (the same with filters on intermediate and destination node ages) are fundamentally self-joins of the `FOLLOWS(src, dst)` relation on the middle vertex `b`. A straightforward execution strategy enumerates all matching `(a, b, c)` triples and increments a counter, which makes runtime proportional to the number of length-2 paths produced and amplifies per-row overhead in row-at-a-time execution.

Kuzu/Ladybug excel on `q8`/`q9` because their join-style execution and factorization can avoid fully materializing the intermediate “path explosion.” Conceptually, `q8` can be computed as a sum over `b` of `inDegree(b) * outDegree(b)`, and `q9` becomes a sum of products with filtered degrees, which is much closer to linear in the edge set than in the number of output paths. See the Kuzu research paper for details: https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf.

`lance-graph` (which relies on a DataFusion query plan) still performs well on `q8`/`q9` because the engine is vectorized, columnar, and parallel, and it can stream through large joins and aggregates efficiently when the data is already in Arrow/Lance form. However, unless the optimizer applies a rewrite equivalent to the degree-product formulation, the engine still does work proportional to the number of join matches, which helps explain why it is substantially faster than Neo4j while remaining slower than Kuzu/Ladybug on these path-counting queries.

## Inspecting query plans

If you want to dig deeper to analyze query performance, most systems provide a way to inspect the plan that the engine chose. This can be useful for validating join order, predicate pushdown, and whether limits and aggregates are applied early.

- Neo4j supports `EXPLAIN` (plan without execution) and `PROFILE` (plan plus runtime counters). You can prepend `EXPLAIN` or `PROFILE` to any Cypher query and inspect the operator tree and row counts.
- Kuzu supports `EXPLAIN` for Cypher queries. You can run `EXPLAIN MATCH ... RETURN ...;` and inspect the returned plan text.
- Ladybug uses the same Cypher dialect as Kuzu for these benchmarks, and the same `EXPLAIN` approach is typically applicable for inspecting its plan output.
- `lance-graph` can produce the query graph plan and the underlying DataFusion logical and physical plans. The `lance_graph` Python API exposes this via `CypherQuery(...).with_config(cfg).explain(datasets)`, which returns the plan as a string for the provided in-memory datasets.
